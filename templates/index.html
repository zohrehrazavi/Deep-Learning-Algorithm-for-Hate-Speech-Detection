<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Hate Speech Detection</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <link
      rel="stylesheet"
      href="{{ url_for('static', filename='style.css') }}"
    />
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <style>
      .metadata-badge {
        font-size: 0.8em;
        margin-right: 5px;
        padding: 3px 8px;
      }
      .rule-based-badge {
        background-color: #17a2b8;
        color: white;
      }
      .context-badge {
        background-color: #6c757d;
        color: white;
      }
      .semantic-badge {
        background-color: #28a745;
        color: white;
      }
    </style>
  </head>
  <body>
    <div class="container mt-5">
      <div class="row justify-content-center">
        <div class="col-md-8">
          <div class="card">
            <div class="card-header bg-primary text-white">
              <h2 class="mb-0">Hate Speech Detection System</h2>
              <button
                type="button"
                class="btn btn-link text-white"
                data-bs-toggle="modal"
                data-bs-target="#howItWorksModal"
              >
                How it works?
              </button>
            </div>
            <div class="card-body">
              <form method="POST" action="{{ url_for('classify') }}">
                <div class="mb-3">
                  <label for="text" class="form-label">
                    Enter text to analyze:
                  </label>
                  <textarea
                    class="form-control"
                    id="text"
                    name="text"
                    rows="5"
                    required
                  >
{{ input_text or '' }}</textarea
                  >
                  <div class="form-text">
                    Enter any text or tweet to check if it contains hate speech
                    or offensive content.
                  </div>
                </div>
                <button type="submit" class="btn btn-primary">Analyze</button>
              </form>

              {% if baseline_result or lstm_result or transformer_result %}
              <hr />
              <h4>Results:</h4>

              {% if baseline_result %}
              <div class="card mb-3">
                <div class="card-header bg-secondary text-white">
                  Baseline (Rules + Classical ML)
                </div>
                <div class="card-body">
                  <h5>
                    {{ baseline_result.label }} — {{
                    '%.1f'|format(baseline_result.confidence) }}%
                  </h5>
                  <p>{{ baseline_result.explanation }}</p>
                  {% if baseline_result.metadata %}
                  <div class="metadata-section mb-2">
                    {% if baseline_result.metadata.is_rule_based %}
                    <span
                      class="badge metadata-badge rule-based-badge"
                      data-bs-toggle="tooltip"
                      title="Classification based on predefined rules"
                    >
                      Rule-based
                    </span>
                    {% endif %} {% if baseline_result.metadata.flagged_context
                    %}
                    <span
                      class="badge metadata-badge context-badge"
                      data-bs-toggle="tooltip"
                      title="Context disclaimer detected"
                    >
                      Context: {{ baseline_result.metadata.flagged_context }}
                    </span>
                    {% endif %} {% if baseline_result.metadata.semantic_alert %}
                    <span
                      class="badge metadata-badge semantic-badge"
                      data-bs-toggle="tooltip"
                      title="Semantic patterns detected"
                    >
                      Semantic Alert
                    </span>
                    {% endif %}
                  </div>
                  {% endif %}
                </div>
              </div>
              {% endif %} {% if lstm_result %}
              <div class="card mb-3">
                <div class="card-header bg-primary text-white">
                  Deep Learning: BiLSTM-Attention
                </div>
                <div class="card-body">
                  {% if "unavailable" in lstm_result.model_name.lower() %}
                  <span class="badge bg-warning text-dark">
                    Model not trained
                  </span>
                  {% else %}
                  <h5>
                    {{ lstm_result.label }} — {{
                    '%.1f'|format(lstm_result.confidence) }}%
                  </h5>
                  <p>Model: {{ lstm_result.model_name }}</p>
                  {% endif %}
                </div>
              </div>
              {% endif %} {% if transformer_result %}
              <div class="card mb-3">
                <div class="card-header bg-info text-white">
                  Deep Learning: DistilBERT
                </div>
                <div class="card-body">
                  {% if "unavailable" in transformer_result.model_name.lower()
                  %}
                  <span class="badge bg-warning text-dark">
                    Model not trained
                  </span>
                  {% else %}
                  <h5>
                    {{ transformer_result.label }} — {{
                    '%.1f'|format(transformer_result.confidence) }}%
                  </h5>
                  <p>Model: {{ transformer_result.model_name }}</p>
                  {% endif %}
                </div>
              </div>
              {% endif %} {% endif %}
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- How it Works Modal -->
    <div
      class="modal fade"
      id="howItWorksModal"
      tabindex="-1"
      aria-labelledby="howItWorksModalLabel"
      aria-hidden="true"
    >
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="modal-header">
            <h5 class="modal-title" id="howItWorksModalLabel">How it Works</h5>
            <button
              type="button"
              class="btn-close"
              data-bs-dismiss="modal"
              aria-label="Close"
            ></button>
          </div>
          <div class="modal-body">
            <p>
              This system uses multiple machine learning and deep learning
              models to detect hate speech and offensive content:
            </p>
            <ul>
              <li>
                <strong>Baseline:</strong>
                Combines rule-based detection with classical ML models (Naive
                Bayes, Logistic Regression)
              </li>
              <li>
                <strong>BiLSTM-Attention:</strong>
                Deep learning model using bidirectional LSTM with attention
                mechanism
              </li>
              <li>
                <strong>DistilBERT:</strong>
                Transformer-based model fine-tuned for hate speech detection
              </li>
            </ul>
            <p>
              The system analyzes your text and provides classification results
              from each model, along with confidence scores.
            </p>
          </div>
          <div class="modal-footer">
            <button
              type="button"
              class="btn btn-secondary"
              data-bs-dismiss="modal"
            >
              Close
            </button>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
